@article{jegham-2020,
	title = {Vision-based human action recognition: An overview and real world challenges},
	volume = {32},
	issn = {26662817},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S174228761930283X},
	doi = {10.1016/j.fsidi.2019.200901},
	shorttitle = {Vision-based human action recognition},
	abstract = {Within a large range of applications in computer vision, Human Action Recognition has become one of the most attractive research ﬁelds. Ambiguities in recognizing actions does not only come from the difﬁculty to deﬁne the motion of body parts, but also from many other challenges related to real world problems such as camera motion, dynamic background, and bad weather conditions. There has been little research work in the real world conditions of human action recognition systems, which encourages us to seriously search in this application domain. Although a plethora of robust approaches have been introduced in the literature, they are still insufﬁcient to fully cover the challenges. To quantitatively and qualitatively compare the performance of these methods, public datasets that present various actions under several conditions and constraints are recorded. In this paper, we investigate an overview of the existing methods according to the kind of issue they address. Moreover, we present a comparison of the existing datasets introduced for the human action recognition ﬁeld.},
	pages = {200901},
	journaltitle = {Forensic Science International: Digital Investigation},
	shortjournal = {Forensic Science International: Digital Investigation},
	author = {Jegham, Imen and Ben Khalifa, Anouar and Alouani, Ihsen and Mahjoub, Mohamed Ali},
	urldate = {2020-07-02},
	date = {2020-03},
	langid = {english},
	keywords = {done, selected, review},
	file = {Jegham et al. - 2020 - Vision-based human action recognition An overview.pdf:/home/zampolo/Zotero/storage/BHU3L7TY/Jegham et al. - 2020 - Vision-based human action recognition An overview.pdf:application/pdf}
}

@article{hussain-2020,
	title = {A review and categorization of techniques on device-free human activity recognition},
	volume = {167},
	issn = {10848045},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1084804520302125},
	doi = {10.1016/j.jnca.2020.102738},
	abstract = {Human activity recognition has gained importance in recent years due to its applications in various ﬁelds such as health, security and surveillance, entertainment, and intelligent environments. A signiﬁcant amount of work has been done on human activity recognition and researchers have leveraged diﬀerent approaches, such as wearable, object-tagged, and device-free, to recognize human activities. In this article, we present a comprehensive survey of the work conducted over the 10-year period of 2010–2019 in various areas of human activity recognition with main focus on device-free solutions. The device-free approach is becoming very popular due to the fact that the subject is not required to carry anything. Instead, the environment is tagged with devices to capture the required information. We propose a new taxonomy for categorizing the research work conducted in the ﬁeld of activity recognition and divide the existing literature into three sub-areas: action-based, motion-based, and interactionbased. We further divide these areas into ten diﬀerent sub-topics and present the latest research works in these sub-topics. Unlike previous surveys which focus only on one type of activities, to the best of our knowledge, we cover all the sub-areas in activity recognition and provide a comparison of the latest research work in these subareas. Speciﬁcally, we discuss the key attributes and design approaches for the work presented. Then we provide extensive analysis based on 10 important metrics, to present a comprehensive overview of the state-of-the-art techniques and trends in diﬀerent sub-areas of device-free human activity recognition. In the end, we discuss open research issues and propose future research directions in the ﬁeld of human activity recognition.},
	pages = {102738},
	journaltitle = {Journal of Network and Computer Applications},
	shortjournal = {Journal of Network and Computer Applications},
	author = {Hussain, Zawar and Sheng, Quan Z. and Zhang, Wei Emma},
	urldate = {2020-07-02},
	date = {2020-10},
	langid = {english},
	keywords = {done, selected, review},
	file = {Hussain et al. - 2020 - A review and categorization of techniques on devic.pdf:/home/zampolo/Zotero/storage/PZCGRYEW/Hussain et al. - 2020 - A review and categorization of techniques on devic.pdf:application/pdf}
}

@article{yao-2019,
	title = {A review of Convolutional-Neural-Network-based action recognition},
	volume = {118},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865518302058},
	doi = {10.1016/j.patrec.2018.05.018},
	abstract = {Video action recognition is widely applied in video indexing, intelligent surveillance, multimedia understanding, and other ﬁelds. Recently, it was greatly improved by incorporating the learning of deep information using Convolutional Neural Network ({CNN}). This motivated us to review the notable {CNN}-based action recognition works. Because {CNN} is primarily designed to extract 2D spatial features from still image and videos are naturally viewed as 3D spatiotemporal signals, the core issue of extending the {CNN} from image to video is temporal information exploitation. We divide the solutions for exploiting temporal information exploration into three strategies: 1) 3D {CNN}; 2) taking the motion-related information as the {CNN} input; and 3) fusion. In this paper, we present a comprehensive review of the {CNN}-based action recognition methods according to these strategies. We also discuss the action recognition performance on recent large-scale benchmarks and the limitations and future research directions of {CNN}-based action recognition. This paper offers an objective and clear review of {CNN}-based action recognition and provides a guide for future research.},
	pages = {14--22},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Yao, Guangle and Lei, Tao and Zhong, Jiandan},
	urldate = {2020-07-21},
	date = {2019-02},
	langid = {english},
	keywords = {done, selected, review},
	file = {Yao et al. - 2019 - A review of Convolutional-Neural-Network-based act.pdf:/home/zampolo/Zotero/storage/UTAMVJM9/Yao et al. - 2019 - A review of Convolutional-Neural-Network-based act.pdf:application/pdf}
}

@article{kongr-2018,
	title = {Human Action Recognition and Prediction: A Survey},
	url = {http://arxiv.org/abs/1806.11230},
	shorttitle = {Human Action Recognition and Prediction},
	abstract = {Derived from rapid advances in computer vision and machine learning, video analysis tasks have been moving from inferring the present state to predicting the future state. Vision-based action recognition and prediction from videos are such tasks, where action recognition is to infer human actions (present state) based upon complete action executions, and action prediction to predict human actions (future state) based upon incomplete action executions. These two tasks have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as visual surveillance, autonomous driving vehicle, entertainment, and video retrieval, etc. Many attempts have been devoted in the last a few decades in order to build a robust and effective framework for action recognition and prediction. In this paper, we survey the complete state-of-the-art techniques in the action recognition and prediction. Existing models, popular algorithms, technical difﬁculties, popular action databases, evaluation protocols, and promising future directions are also provided with systematic discussions.},
	journaltitle = {{arXiv}:1806.11230 [cs]},
	author = {Kong, Yu and Fu, Yun},
	urldate = {2020-06-24},
	date = {2018-07-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.11230},
	keywords = {done, selected, review},
	file = {1806.11230(1).pdf:/home/zampolo/engenharia/artigos/activity-recognition/1806.11230(1).pdf:application/pdf}
}

@article{herath-2017,
	title = {Going deeper into action recognition: A survey},
	volume = {60},
	issn = {02628856},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885617300343},
	doi = {10.1016/j.imavis.2017.01.010},
	shorttitle = {Going deeper into action recognition},
	abstract = {Understanding human actions in visual data is tied to advances in complementary research areas including object recognition, human dynamics, domain adaptation and semantic segmentation. Over the last decade, human action analysis evolved from earlier schemes that are often limited to controlled environments to nowadays advanced solutions that can learn from millions of videos and apply to almost all daily activities. Given the broad range of applications from video surveillance to human–computer interaction, scientiﬁc milestones in action recognition are achieved more rapidly, eventually leading to the demise of what used to be good in a short time. This motivated us to provide a comprehensive review of the notable steps taken towards recognizing human actions. To this end, we start our discussion with the pioneering methods that use handcrafted representations, and then, navigate into the realm of deep learning based approaches. We aim to remain objective throughout this survey, touching upon encouraging improvements as well as inevitable fallbacks, in the hope of raising fresh questions and motivating new research directions for the reader.},
	pages = {4--21},
	journaltitle = {Image and Vision Computing},
	shortjournal = {Image and Vision Computing},
	author = {Herath, Samitha and Harandi, Mehrtash and Porikli, Fatih},
	urldate = {2020-07-17},
	date = {2017-04},
	langid = {english},
	keywords = {done, selected, review},
	file = {Herath et al. - 2017 - Going deeper into action recognition A survey.pdf:/home/zampolo/Zotero/storage/HQ9WA5GR/Herath et al. - 2017 - Going deeper into action recognition A survey.pdf:application/pdf}
}
